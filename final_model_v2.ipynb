{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df):\n",
    "    msk = np.random.rand(len(df)) < 0.9\n",
    "    train = df[msk].reset_index()\n",
    "    valid = df[~msk].reset_index()\n",
    "    return train.drop(columns=['index']), valid.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model_and_data(train):\n",
    "    train_df = encode_data(train, train=None)\n",
    "    num_users = len(train.user_id.unique())\n",
    "    num_items = len(train.item_id.unique())\n",
    "    model = MF(num_users, num_items)\n",
    "    return model, train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_col(col, train_col=None):\n",
    "    \"\"\"Encodes a pandas column with continous ids. \n",
    "    \"\"\"\n",
    "    if train_col is not None:\n",
    "        uniq = train_col.unique()\n",
    "    else:\n",
    "        uniq = col.unique()\n",
    "    name2idx = {o:i for i,o in enumerate(uniq)}\n",
    "    return name2idx, np.array([name2idx.get(x, -1) for x in col]), len(uniq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(df, train=None):\n",
    "    \"\"\" Encodes rating data with continous user and movie ids. \n",
    "    If train is provided, encodes df with the same encoding as train.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    for col_name in [\"user_id\", \"item_id\"]:\n",
    "        train_col = None\n",
    "        if train is not None:\n",
    "            train_col = train[col_name]\n",
    "        _,col,_ = proc_col(df[col_name], train_col)\n",
    "        df[col_name] = col\n",
    "        df = df[df[col_name] >= 0]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_item_features(df, train=None):\n",
    "    \"\"\" Encodes rating data with continous user and movie ids. \n",
    "    If train is provided, encodes df with the same encoding as train.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    for col_name in [\"item_feature_id\"]:\n",
    "        train_col = None\n",
    "        if train is not None:\n",
    "            train_col = train[col_name]\n",
    "        _,col,_ = proc_col(df[col_name], train_col)\n",
    "        df[col_name] = col\n",
    "        df = df[df[col_name] >= 0]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_samples(df, k, all_users, all_items, user_context_features):\n",
    "    sample_df = {'user_id':np.empty(df.shape[0]*k),\n",
    "        'item_id':np.empty(df.shape[0]*k),\n",
    "        'rating':np.empty(df.shape[0]*k),\n",
    "        'context_feature_id':np.empty(df.shape[0]*k)}\n",
    "    for user in all_users:\n",
    "        split_index = np.where(df[:,0] == user)[0]\n",
    "        user_df = df[split_index[0]:split_index[-1]+1]\n",
    "        \n",
    "        zero_samples = sample_zeros(user_df, k, all_items)\n",
    "        user_col = np.full(user_df.shape[0]*k, user)\n",
    "        zero_col = np.zeros(user_df.shape[0]*k)\n",
    "\n",
    "        context_features = np.random.choice(user_context_features[user_df[0,0]],\n",
    "         user_df.shape[0]*k)\n",
    "\n",
    "        # sample_df = {'user_id':user_col, 'item_id':zero_samples, 'rating':zero_col}\n",
    "        sample_df['user_id'][split_index[0]:split_index[-1]+1] = user_col\n",
    "        sample_df['item_id'][split_index[0]:split_index[-1]+1] = zero_samples\n",
    "        sample_df['rating'][split_index[0]:split_index[-1]+1] = zero_col\n",
    "        sample_df['context_feature_id'][split_index[0]:split_index[-1]+1] = context_features\n",
    "\n",
    "\n",
    "    return pd.DataFrame(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_zeros(user_df, k, all_items):\n",
    "    sample = []\n",
    "    \n",
    "    while len(sample) < k*len(user_df[:,1]):\n",
    "        raw_samp = np.random.randint(0, len(all_items))\n",
    "        if raw_samp not in user_df[:,1] and raw_samp not in sample:\n",
    "            sample.append(raw_samp)\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('training.csv')\n",
    "train_df['rating'] = 1\n",
    "items_id_df = pd.read_csv('item_feature.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling = train_df.to_numpy()\n",
    "all_users = pd.unique(sampling[:,0])\n",
    "all_items = pd.unique(sampling[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_samples = add_samples(sampling, 1, all_users, all_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_feature_id_dict = {}\n",
    "for user in train_df['user_id'].unique():\n",
    "    context_feature_id_dict[user] = train_df[train_df['user_id'] == user]['context_feature_id'].sample(1).values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_samples['context_feature_id'] = negative_samples['user_id'].map(context_feature_id_dict)\n",
    "train_df_new = pd.concat([negative_samples, train_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>context_feature_id</th>\n",
       "      <th>item_feature_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>22331</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3323</td>\n",
       "      <td>22331</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6421</td>\n",
       "      <td>22331</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9189</td>\n",
       "      <td>22331</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19281</td>\n",
       "      <td>22331</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940485</th>\n",
       "      <td>198093</td>\n",
       "      <td>38949</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940486</th>\n",
       "      <td>198439</td>\n",
       "      <td>39733</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940487</th>\n",
       "      <td>198921</td>\n",
       "      <td>39831</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940488</th>\n",
       "      <td>198921</td>\n",
       "      <td>37989</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940489</th>\n",
       "      <td>199208</td>\n",
       "      <td>38350</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1940490 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  item_id  rating  context_feature_id  item_feature_id\n",
       "0              0    22331       0                   3              124\n",
       "1           3323    22331       0                   2              124\n",
       "2           6421    22331       0                   0              124\n",
       "3           9189    22331       0                   2              124\n",
       "4          19281    22331       0                   3              124\n",
       "...          ...      ...     ...                 ...              ...\n",
       "1940485   198093    38949       1                   1              142\n",
       "1940486   198439    39733       1                   2              138\n",
       "1940487   198921    39831       1                   2               94\n",
       "1940488   198921    37989       1                   2               94\n",
       "1940489   199208    38350       1                   2              148\n",
       "\n",
       "[1940490 rows x 5 columns]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df = train_df_new.merge(items_id_df, left_on='item_id', right_on='item_id')\n",
    "training_df = training_df.astype('int')\n",
    "training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>context_feature_id</th>\n",
       "      <th>item_feature_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>22331</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3323</td>\n",
       "      <td>22331</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6421</td>\n",
       "      <td>22331</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9189</td>\n",
       "      <td>22331</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19281</td>\n",
       "      <td>22331</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940485</th>\n",
       "      <td>198093</td>\n",
       "      <td>38949</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940486</th>\n",
       "      <td>198439</td>\n",
       "      <td>39733</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940487</th>\n",
       "      <td>198921</td>\n",
       "      <td>39831</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940488</th>\n",
       "      <td>198921</td>\n",
       "      <td>37989</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940489</th>\n",
       "      <td>199208</td>\n",
       "      <td>38350</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1940490 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  item_id  rating  context_feature_id  item_feature_id\n",
       "0              0    22331       0                   3              124\n",
       "1           3323    22331       0                   2              124\n",
       "2           6421    22331       0                   0              124\n",
       "3           9189    22331       0                   2              124\n",
       "4          19281    22331       0                   3              124\n",
       "...          ...      ...     ...                 ...              ...\n",
       "1940485   198093    38949       1                   1              142\n",
       "1940486   198439    39733       1                   2              138\n",
       "1940487   198921    39831       1                   2               94\n",
       "1940488   198921    37989       1                   2               94\n",
       "1940489   199208    38350       1                   2              148\n",
       "\n",
       "[1940490 rows x 5 columns]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Factorization Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF_bias(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_size=100, seed=None):\n",
    "        super(MF_bias, self).__init__()\n",
    "        if seed is None:\n",
    "            torch.manual_seed(np.random.randint(1,100000))\n",
    "        else:\n",
    "            torch.manual_seed(seed)\n",
    "        self.user_emb = nn.Embedding(num_users, emb_size)\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_size)\n",
    "        self.item_bias = nn.Embedding(num_items, 1)\n",
    "        \n",
    "        self.user_emb.weight.data.uniform_(0,0.05)\n",
    "        self.item_emb.weight.data.uniform_(0,0.05)\n",
    "        self.user_bias.weight.data.uniform_(-0.01,0.01)\n",
    "        self.item_bias.weight.data.uniform_(-0.01,0.01)\n",
    "\n",
    "    def forward(self, u, v):\n",
    "        U = self.user_emb(u)\n",
    "        V = self.item_emb(v)\n",
    "        b_u = self.user_bias(u).squeeze()\n",
    "        b_v = self.item_bias(v).squeeze()\n",
    "        return ((U*V).sum(1) + b_u + b_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epocs(model, epochs=10, lr=0.01, wd=0.0):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        users = torch.LongTensor(train_df.user_id.values)  \n",
    "        items = torch.LongTensor(train_df.item_id.values) \n",
    "        ratings = torch.FloatTensor(train_df.rating.values)  \n",
    "    \n",
    "        y_hat = model(users, items)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, ratings)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        testloss, acc = valid_loss(model)\n",
    "        print(\"train loss %.3f valid loss %.3f accuracy %.3f\" % (loss.item(), testloss, acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epocs_full(model, epochs=10, lr=0.01, wd=0.0):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        users = torch.LongTensor(training_df.user_id.values)  \n",
    "        items = torch.LongTensor(training_df.item_id.values) \n",
    "        ratings = torch.FloatTensor(training_df.rating.values)  \n",
    "    \n",
    "        y_hat = model(users, items)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, ratings)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Trained!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_loss(model):\n",
    "    model.eval()\n",
    "    users = torch.LongTensor(valid_df.user_id.values)  \n",
    "    items = torch.LongTensor(valid_df.item_id.values) \n",
    "    ratings = torch.FloatTensor(valid_df.rating.values)  \n",
    "    y_hat = model(users, items)\n",
    "    loss = F.binary_cross_entropy_with_logits(y_hat, ratings)\n",
    "    preds = (torch.sigmoid(y_hat) > 0.5).float()\n",
    "    return loss.item(), balanced_accuracy_score(ratings, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>context_feature_id</th>\n",
       "      <th>item_feature_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>22331</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3323</td>\n",
       "      <td>22331</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6421</td>\n",
       "      <td>22331</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9189</td>\n",
       "      <td>22331</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19281</td>\n",
       "      <td>22331</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940485</th>\n",
       "      <td>198093</td>\n",
       "      <td>38949</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940486</th>\n",
       "      <td>198439</td>\n",
       "      <td>39733</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940487</th>\n",
       "      <td>198921</td>\n",
       "      <td>39831</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940488</th>\n",
       "      <td>198921</td>\n",
       "      <td>37989</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940489</th>\n",
       "      <td>199208</td>\n",
       "      <td>38350</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1940490 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  item_id  rating  context_feature_id  item_feature_id\n",
       "0              0    22331       0                   3              124\n",
       "1           3323    22331       0                   2              124\n",
       "2           6421    22331       0                   0              124\n",
       "3           9189    22331       0                   2              124\n",
       "4          19281    22331       0                   3              124\n",
       "...          ...      ...     ...                 ...              ...\n",
       "1940485   198093    38949       1                   1              142\n",
       "1940486   198439    39733       1                   2              138\n",
       "1940487   198921    39831       1                   2               94\n",
       "1940488   198921    37989       1                   2               94\n",
       "1940489   199208    38350       1                   2              148\n",
       "\n",
       "[1940490 rows x 5 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df = pd.read_feather('training_sampled')\n",
    "training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = training_df['user_id'].max()\n",
    "num_items = training_df['item_id'].unique().max()\n",
    "model = MF_bias(num_users+1, num_items+1, emb_size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.693 valid loss 0.745 accuracy 0.184\n",
      "train loss 0.711 valid loss 0.650 accuracy 0.866\n",
      "train loss 0.627 valid loss 0.602 accuracy 0.869\n",
      "train loss 0.594 valid loss 0.577 accuracy 0.872\n",
      "train loss 0.576 valid loss 0.546 accuracy 0.873\n",
      "train loss 0.546 valid loss 0.525 accuracy 0.872\n",
      "train loss 0.519 valid loss 0.518 accuracy 0.867\n",
      "train loss 0.505 valid loss 0.517 accuracy 0.863\n",
      "train loss 0.498 valid loss 0.509 accuracy 0.862\n",
      "train loss 0.489 valid loss 0.494 accuracy 0.867\n",
      "train loss 0.475 valid loss 0.479 accuracy 0.869\n",
      "train loss 0.463 valid loss 0.468 accuracy 0.870\n",
      "train loss 0.455 valid loss 0.462 accuracy 0.872\n",
      "train loss 0.452 valid loss 0.459 accuracy 0.873\n",
      "train loss 0.450 valid loss 0.458 accuracy 0.872\n",
      "train loss 0.448 valid loss 0.460 accuracy 0.871\n",
      "train loss 0.447 valid loss 0.463 accuracy 0.870\n",
      "train loss 0.447 valid loss 0.467 accuracy 0.869\n",
      "train loss 0.449 valid loss 0.468 accuracy 0.869\n",
      "train loss 0.450 valid loss 0.466 accuracy 0.869\n",
      "train loss 0.450 valid loss 0.461 accuracy 0.870\n",
      "train loss 0.448 valid loss 0.457 accuracy 0.871\n",
      "train loss 0.446 valid loss 0.454 accuracy 0.871\n",
      "train loss 0.444 valid loss 0.451 accuracy 0.872\n",
      "train loss 0.443 valid loss 0.449 accuracy 0.871\n",
      "train loss 0.440 valid loss 0.448 accuracy 0.870\n",
      "train loss 0.438 valid loss 0.447 accuracy 0.870\n",
      "train loss 0.435 valid loss 0.445 accuracy 0.870\n",
      "train loss 0.433 valid loss 0.443 accuracy 0.870\n",
      "train loss 0.431 valid loss 0.440 accuracy 0.870\n",
      "train loss 0.428 valid loss 0.437 accuracy 0.870\n",
      "train loss 0.426 valid loss 0.434 accuracy 0.871\n",
      "train loss 0.424 valid loss 0.432 accuracy 0.871\n",
      "train loss 0.422 valid loss 0.431 accuracy 0.871\n",
      "train loss 0.421 valid loss 0.431 accuracy 0.871\n",
      "train loss 0.420 valid loss 0.432 accuracy 0.870\n",
      "train loss 0.420 valid loss 0.432 accuracy 0.870\n",
      "train loss 0.419 valid loss 0.432 accuracy 0.870\n",
      "train loss 0.419 valid loss 0.433 accuracy 0.870\n",
      "train loss 0.420 valid loss 0.432 accuracy 0.871\n",
      "train loss 0.420 valid loss 0.432 accuracy 0.871\n",
      "train loss 0.420 valid loss 0.432 accuracy 0.871\n",
      "train loss 0.421 valid loss 0.432 accuracy 0.871\n",
      "train loss 0.421 valid loss 0.433 accuracy 0.871\n",
      "train loss 0.421 valid loss 0.434 accuracy 0.870\n",
      "train loss 0.422 valid loss 0.434 accuracy 0.870\n",
      "train loss 0.422 valid loss 0.434 accuracy 0.870\n",
      "train loss 0.422 valid loss 0.434 accuracy 0.871\n",
      "train loss 0.422 valid loss 0.434 accuracy 0.871\n",
      "train loss 0.422 valid loss 0.433 accuracy 0.871\n",
      "train loss 0.422 valid loss 0.433 accuracy 0.871\n",
      "train loss 0.422 valid loss 0.432 accuracy 0.871\n",
      "train loss 0.422 valid loss 0.432 accuracy 0.871\n",
      "train loss 0.421 valid loss 0.432 accuracy 0.870\n",
      "train loss 0.421 valid loss 0.432 accuracy 0.870\n",
      "train loss 0.421 valid loss 0.432 accuracy 0.871\n",
      "train loss 0.420 valid loss 0.432 accuracy 0.871\n",
      "train loss 0.420 valid loss 0.431 accuracy 0.871\n",
      "train loss 0.420 valid loss 0.431 accuracy 0.871\n",
      "train loss 0.420 valid loss 0.431 accuracy 0.871\n",
      "train loss 0.420 valid loss 0.431 accuracy 0.871\n",
      "train loss 0.420 valid loss 0.431 accuracy 0.871\n",
      "train loss 0.420 valid loss 0.431 accuracy 0.871\n",
      "train loss 0.420 valid loss 0.432 accuracy 0.871\n",
      "train loss 0.420 valid loss 0.432 accuracy 0.871\n",
      "train loss 0.420 valid loss 0.432 accuracy 0.871\n",
      "train loss 0.420 valid loss 0.432 accuracy 0.871\n",
      "train loss 0.420 valid loss 0.432 accuracy 0.871\n",
      "train loss 0.420 valid loss 0.432 accuracy 0.871\n",
      "train loss 0.420 valid loss 0.432 accuracy 0.871\n",
      "train loss 0.420 valid loss 0.432 accuracy 0.871\n",
      "train loss 0.420 valid loss 0.432 accuracy 0.871\n",
      "train loss 0.420 valid loss 0.432 accuracy 0.871\n",
      "train loss 0.420 valid loss 0.432 accuracy 0.871\n",
      "train loss 0.420 valid loss 0.432 accuracy 0.871\n",
      "train loss 0.420 valid loss 0.432 accuracy 0.871\n",
      "train loss 0.420 valid loss 0.431 accuracy 0.871\n",
      "train loss 0.420 valid loss 0.431 accuracy 0.871\n",
      "train loss 0.420 valid loss 0.431 accuracy 0.871\n",
      "train loss 0.420 valid loss 0.431 accuracy 0.871\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, 80, 0.1, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.420 valid loss 0.430 accuracy 0.871\n",
      "train loss 0.419 valid loss 0.429 accuracy 0.871\n",
      "train loss 0.417 valid loss 0.428 accuracy 0.871\n",
      "train loss 0.416 valid loss 0.427 accuracy 0.871\n",
      "train loss 0.415 valid loss 0.426 accuracy 0.871\n",
      "train loss 0.413 valid loss 0.425 accuracy 0.871\n",
      "train loss 0.412 valid loss 0.424 accuracy 0.871\n",
      "train loss 0.411 valid loss 0.423 accuracy 0.871\n",
      "train loss 0.409 valid loss 0.422 accuracy 0.871\n",
      "train loss 0.408 valid loss 0.421 accuracy 0.871\n",
      "train loss 0.407 valid loss 0.420 accuracy 0.871\n",
      "train loss 0.405 valid loss 0.420 accuracy 0.871\n",
      "train loss 0.404 valid loss 0.418 accuracy 0.871\n",
      "train loss 0.402 valid loss 0.417 accuracy 0.871\n",
      "train loss 0.401 valid loss 0.416 accuracy 0.871\n",
      "train loss 0.400 valid loss 0.415 accuracy 0.871\n",
      "train loss 0.398 valid loss 0.414 accuracy 0.871\n",
      "train loss 0.396 valid loss 0.413 accuracy 0.871\n",
      "train loss 0.395 valid loss 0.412 accuracy 0.871\n",
      "train loss 0.393 valid loss 0.411 accuracy 0.871\n",
      "train loss 0.391 valid loss 0.410 accuracy 0.871\n",
      "train loss 0.389 valid loss 0.408 accuracy 0.872\n",
      "train loss 0.387 valid loss 0.407 accuracy 0.872\n",
      "train loss 0.385 valid loss 0.406 accuracy 0.872\n",
      "train loss 0.383 valid loss 0.404 accuracy 0.872\n",
      "train loss 0.381 valid loss 0.403 accuracy 0.873\n",
      "train loss 0.378 valid loss 0.402 accuracy 0.873\n",
      "train loss 0.376 valid loss 0.400 accuracy 0.873\n",
      "train loss 0.374 valid loss 0.399 accuracy 0.873\n",
      "train loss 0.371 valid loss 0.397 accuracy 0.874\n",
      "train loss 0.368 valid loss 0.395 accuracy 0.874\n",
      "train loss 0.365 valid loss 0.394 accuracy 0.874\n",
      "train loss 0.363 valid loss 0.392 accuracy 0.874\n",
      "train loss 0.360 valid loss 0.390 accuracy 0.874\n",
      "train loss 0.357 valid loss 0.389 accuracy 0.875\n",
      "train loss 0.354 valid loss 0.387 accuracy 0.875\n",
      "train loss 0.351 valid loss 0.385 accuracy 0.875\n",
      "train loss 0.347 valid loss 0.384 accuracy 0.875\n",
      "train loss 0.344 valid loss 0.382 accuracy 0.875\n",
      "train loss 0.341 valid loss 0.380 accuracy 0.876\n",
      "train loss 0.338 valid loss 0.379 accuracy 0.876\n",
      "train loss 0.334 valid loss 0.377 accuracy 0.876\n",
      "train loss 0.331 valid loss 0.376 accuracy 0.876\n",
      "train loss 0.328 valid loss 0.374 accuracy 0.876\n",
      "train loss 0.325 valid loss 0.373 accuracy 0.876\n",
      "train loss 0.322 valid loss 0.371 accuracy 0.876\n",
      "train loss 0.319 valid loss 0.370 accuracy 0.876\n",
      "train loss 0.316 valid loss 0.368 accuracy 0.876\n",
      "train loss 0.313 valid loss 0.367 accuracy 0.876\n",
      "train loss 0.310 valid loss 0.366 accuracy 0.877\n",
      "train loss 0.307 valid loss 0.365 accuracy 0.877\n",
      "train loss 0.304 valid loss 0.363 accuracy 0.877\n",
      "train loss 0.301 valid loss 0.362 accuracy 0.877\n",
      "train loss 0.298 valid loss 0.361 accuracy 0.878\n",
      "train loss 0.296 valid loss 0.360 accuracy 0.878\n",
      "train loss 0.293 valid loss 0.359 accuracy 0.878\n",
      "train loss 0.290 valid loss 0.358 accuracy 0.878\n",
      "train loss 0.288 valid loss 0.357 accuracy 0.879\n",
      "train loss 0.285 valid loss 0.356 accuracy 0.879\n",
      "train loss 0.283 valid loss 0.355 accuracy 0.880\n",
      "train loss 0.281 valid loss 0.354 accuracy 0.880\n",
      "train loss 0.278 valid loss 0.353 accuracy 0.880\n",
      "train loss 0.276 valid loss 0.352 accuracy 0.881\n",
      "train loss 0.274 valid loss 0.351 accuracy 0.882\n",
      "train loss 0.271 valid loss 0.350 accuracy 0.882\n",
      "train loss 0.269 valid loss 0.349 accuracy 0.882\n",
      "train loss 0.267 valid loss 0.349 accuracy 0.883\n",
      "train loss 0.264 valid loss 0.348 accuracy 0.883\n",
      "train loss 0.262 valid loss 0.347 accuracy 0.884\n",
      "train loss 0.260 valid loss 0.346 accuracy 0.884\n",
      "train loss 0.258 valid loss 0.345 accuracy 0.885\n",
      "train loss 0.256 valid loss 0.344 accuracy 0.885\n",
      "train loss 0.253 valid loss 0.343 accuracy 0.886\n",
      "train loss 0.251 valid loss 0.343 accuracy 0.886\n",
      "train loss 0.249 valid loss 0.342 accuracy 0.887\n",
      "train loss 0.247 valid loss 0.341 accuracy 0.887\n",
      "train loss 0.245 valid loss 0.340 accuracy 0.887\n",
      "train loss 0.243 valid loss 0.339 accuracy 0.888\n",
      "train loss 0.240 valid loss 0.338 accuracy 0.888\n",
      "train loss 0.238 valid loss 0.338 accuracy 0.888\n",
      "train loss 0.236 valid loss 0.337 accuracy 0.889\n",
      "train loss 0.234 valid loss 0.336 accuracy 0.889\n",
      "train loss 0.232 valid loss 0.335 accuracy 0.890\n",
      "train loss 0.230 valid loss 0.335 accuracy 0.890\n",
      "train loss 0.228 valid loss 0.334 accuracy 0.890\n",
      "train loss 0.226 valid loss 0.333 accuracy 0.891\n",
      "train loss 0.224 valid loss 0.332 accuracy 0.891\n",
      "train loss 0.222 valid loss 0.332 accuracy 0.891\n",
      "train loss 0.221 valid loss 0.331 accuracy 0.891\n",
      "train loss 0.219 valid loss 0.330 accuracy 0.892\n",
      "train loss 0.217 valid loss 0.330 accuracy 0.892\n",
      "train loss 0.215 valid loss 0.329 accuracy 0.892\n",
      "train loss 0.214 valid loss 0.328 accuracy 0.892\n",
      "train loss 0.212 valid loss 0.328 accuracy 0.893\n",
      "train loss 0.210 valid loss 0.327 accuracy 0.893\n",
      "train loss 0.209 valid loss 0.326 accuracy 0.893\n",
      "train loss 0.207 valid loss 0.326 accuracy 0.894\n",
      "train loss 0.206 valid loss 0.325 accuracy 0.894\n",
      "train loss 0.204 valid loss 0.325 accuracy 0.894\n",
      "train loss 0.203 valid loss 0.324 accuracy 0.894\n",
      "train loss 0.201 valid loss 0.323 accuracy 0.895\n",
      "train loss 0.200 valid loss 0.323 accuracy 0.895\n",
      "train loss 0.199 valid loss 0.322 accuracy 0.895\n",
      "train loss 0.198 valid loss 0.322 accuracy 0.895\n",
      "train loss 0.196 valid loss 0.321 accuracy 0.895\n",
      "train loss 0.195 valid loss 0.321 accuracy 0.896\n",
      "train loss 0.194 valid loss 0.320 accuracy 0.896\n",
      "train loss 0.193 valid loss 0.320 accuracy 0.896\n",
      "train loss 0.192 valid loss 0.319 accuracy 0.896\n",
      "train loss 0.191 valid loss 0.319 accuracy 0.896\n",
      "train loss 0.190 valid loss 0.318 accuracy 0.896\n",
      "train loss 0.189 valid loss 0.318 accuracy 0.896\n",
      "train loss 0.188 valid loss 0.317 accuracy 0.897\n",
      "train loss 0.187 valid loss 0.317 accuracy 0.897\n",
      "train loss 0.186 valid loss 0.317 accuracy 0.897\n",
      "train loss 0.185 valid loss 0.316 accuracy 0.897\n",
      "train loss 0.184 valid loss 0.316 accuracy 0.897\n",
      "train loss 0.183 valid loss 0.315 accuracy 0.898\n",
      "train loss 0.183 valid loss 0.315 accuracy 0.898\n",
      "train loss 0.182 valid loss 0.314 accuracy 0.898\n",
      "train loss 0.181 valid loss 0.314 accuracy 0.898\n",
      "train loss 0.180 valid loss 0.314 accuracy 0.898\n",
      "train loss 0.180 valid loss 0.313 accuracy 0.898\n",
      "train loss 0.179 valid loss 0.313 accuracy 0.898\n",
      "train loss 0.178 valid loss 0.313 accuracy 0.898\n",
      "train loss 0.178 valid loss 0.312 accuracy 0.898\n",
      "train loss 0.177 valid loss 0.312 accuracy 0.899\n",
      "train loss 0.177 valid loss 0.311 accuracy 0.899\n",
      "train loss 0.176 valid loss 0.311 accuracy 0.899\n",
      "train loss 0.175 valid loss 0.311 accuracy 0.899\n",
      "train loss 0.175 valid loss 0.310 accuracy 0.899\n",
      "train loss 0.174 valid loss 0.310 accuracy 0.899\n",
      "train loss 0.174 valid loss 0.310 accuracy 0.899\n",
      "train loss 0.173 valid loss 0.310 accuracy 0.899\n",
      "train loss 0.173 valid loss 0.309 accuracy 0.899\n",
      "train loss 0.172 valid loss 0.309 accuracy 0.899\n",
      "train loss 0.172 valid loss 0.309 accuracy 0.900\n",
      "train loss 0.171 valid loss 0.308 accuracy 0.900\n",
      "train loss 0.171 valid loss 0.308 accuracy 0.900\n",
      "train loss 0.170 valid loss 0.308 accuracy 0.900\n",
      "train loss 0.170 valid loss 0.307 accuracy 0.900\n",
      "train loss 0.170 valid loss 0.307 accuracy 0.900\n",
      "train loss 0.169 valid loss 0.307 accuracy 0.900\n",
      "train loss 0.169 valid loss 0.307 accuracy 0.900\n",
      "train loss 0.168 valid loss 0.306 accuracy 0.900\n",
      "train loss 0.168 valid loss 0.306 accuracy 0.900\n",
      "train loss 0.168 valid loss 0.306 accuracy 0.900\n",
      "train loss 0.167 valid loss 0.306 accuracy 0.900\n",
      "train loss 0.167 valid loss 0.305 accuracy 0.901\n",
      "train loss 0.167 valid loss 0.305 accuracy 0.901\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, 150, lr=0.005, wd=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.166 valid loss 0.305 accuracy 0.901\n",
      "train loss 0.166 valid loss 0.304 accuracy 0.901\n",
      "train loss 0.165 valid loss 0.304 accuracy 0.901\n",
      "train loss 0.165 valid loss 0.303 accuracy 0.901\n",
      "train loss 0.164 valid loss 0.303 accuracy 0.901\n",
      "train loss 0.164 valid loss 0.302 accuracy 0.901\n",
      "train loss 0.163 valid loss 0.302 accuracy 0.901\n",
      "train loss 0.163 valid loss 0.302 accuracy 0.902\n",
      "train loss 0.162 valid loss 0.301 accuracy 0.902\n",
      "train loss 0.162 valid loss 0.301 accuracy 0.902\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, 10, lr=0.005, wd=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.162 valid loss 0.300 accuracy 0.902\n",
      "train loss 0.161 valid loss 0.300 accuracy 0.902\n",
      "train loss 0.161 valid loss 0.300 accuracy 0.902\n",
      "train loss 0.160 valid loss 0.299 accuracy 0.902\n",
      "train loss 0.160 valid loss 0.299 accuracy 0.902\n",
      "train loss 0.160 valid loss 0.298 accuracy 0.902\n",
      "train loss 0.159 valid loss 0.298 accuracy 0.902\n",
      "train loss 0.159 valid loss 0.298 accuracy 0.902\n",
      "train loss 0.158 valid loss 0.297 accuracy 0.902\n",
      "train loss 0.158 valid loss 0.297 accuracy 0.903\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, 10, lr=0.005, wd=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.158 valid loss 0.297 accuracy 0.903\n",
      "train loss 0.157 valid loss 0.296 accuracy 0.903\n",
      "train loss 0.157 valid loss 0.296 accuracy 0.903\n",
      "train loss 0.157 valid loss 0.296 accuracy 0.903\n",
      "train loss 0.156 valid loss 0.295 accuracy 0.903\n",
      "train loss 0.156 valid loss 0.295 accuracy 0.903\n",
      "train loss 0.156 valid loss 0.294 accuracy 0.903\n",
      "train loss 0.155 valid loss 0.294 accuracy 0.903\n",
      "train loss 0.155 valid loss 0.294 accuracy 0.903\n",
      "train loss 0.155 valid loss 0.293 accuracy 0.904\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, 10, lr=0.005, wd=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.154 valid loss 0.293 accuracy 0.904\n",
      "train loss 0.154 valid loss 0.293 accuracy 0.904\n",
      "train loss 0.154 valid loss 0.292 accuracy 0.904\n",
      "train loss 0.154 valid loss 0.292 accuracy 0.904\n",
      "train loss 0.153 valid loss 0.292 accuracy 0.904\n",
      "train loss 0.153 valid loss 0.291 accuracy 0.904\n",
      "train loss 0.153 valid loss 0.291 accuracy 0.904\n",
      "train loss 0.152 valid loss 0.291 accuracy 0.904\n",
      "train loss 0.152 valid loss 0.291 accuracy 0.904\n",
      "train loss 0.152 valid loss 0.290 accuracy 0.904\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, 10, lr=0.005, wd=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.152 valid loss 0.290 accuracy 0.905\n",
      "train loss 0.151 valid loss 0.290 accuracy 0.905\n",
      "train loss 0.151 valid loss 0.289 accuracy 0.905\n",
      "train loss 0.151 valid loss 0.289 accuracy 0.905\n",
      "train loss 0.151 valid loss 0.289 accuracy 0.905\n",
      "train loss 0.150 valid loss 0.288 accuracy 0.905\n",
      "train loss 0.150 valid loss 0.288 accuracy 0.905\n",
      "train loss 0.150 valid loss 0.288 accuracy 0.905\n",
      "train loss 0.150 valid loss 0.288 accuracy 0.905\n",
      "train loss 0.149 valid loss 0.287 accuracy 0.905\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, 10, lr=0.005, wd=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.149 valid loss 0.287 accuracy 0.905\n",
      "train loss 0.149 valid loss 0.287 accuracy 0.905\n",
      "train loss 0.149 valid loss 0.286 accuracy 0.906\n",
      "train loss 0.148 valid loss 0.286 accuracy 0.906\n",
      "train loss 0.148 valid loss 0.286 accuracy 0.906\n",
      "train loss 0.148 valid loss 0.286 accuracy 0.906\n",
      "train loss 0.148 valid loss 0.285 accuracy 0.906\n",
      "train loss 0.148 valid loss 0.285 accuracy 0.906\n",
      "train loss 0.147 valid loss 0.285 accuracy 0.906\n",
      "train loss 0.147 valid loss 0.285 accuracy 0.906\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, 10, lr=0.005, wd=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.147 valid loss 0.284 accuracy 0.906\n",
      "train loss 0.147 valid loss 0.284 accuracy 0.906\n",
      "train loss 0.147 valid loss 0.284 accuracy 0.906\n",
      "train loss 0.146 valid loss 0.284 accuracy 0.906\n",
      "train loss 0.146 valid loss 0.283 accuracy 0.906\n",
      "train loss 0.146 valid loss 0.283 accuracy 0.907\n",
      "train loss 0.146 valid loss 0.283 accuracy 0.907\n",
      "train loss 0.146 valid loss 0.283 accuracy 0.907\n",
      "train loss 0.146 valid loss 0.282 accuracy 0.907\n",
      "train loss 0.145 valid loss 0.282 accuracy 0.907\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, 10, lr=0.005, wd=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.145 valid loss 0.282 accuracy 0.907\n",
      "train loss 0.145 valid loss 0.282 accuracy 0.907\n",
      "train loss 0.145 valid loss 0.282 accuracy 0.907\n",
      "train loss 0.145 valid loss 0.281 accuracy 0.907\n",
      "train loss 0.145 valid loss 0.281 accuracy 0.907\n",
      "train loss 0.144 valid loss 0.281 accuracy 0.907\n",
      "train loss 0.144 valid loss 0.281 accuracy 0.907\n",
      "train loss 0.144 valid loss 0.281 accuracy 0.907\n",
      "train loss 0.144 valid loss 0.280 accuracy 0.907\n",
      "train loss 0.144 valid loss 0.280 accuracy 0.907\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, 10, lr=0.005, wd=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.144 valid loss 0.280 accuracy 0.907\n",
      "train loss 0.144 valid loss 0.280 accuracy 0.908\n",
      "train loss 0.143 valid loss 0.280 accuracy 0.908\n",
      "train loss 0.143 valid loss 0.280 accuracy 0.908\n",
      "train loss 0.143 valid loss 0.279 accuracy 0.908\n",
      "train loss 0.143 valid loss 0.279 accuracy 0.908\n",
      "train loss 0.143 valid loss 0.279 accuracy 0.908\n",
      "train loss 0.143 valid loss 0.279 accuracy 0.908\n",
      "train loss 0.143 valid loss 0.279 accuracy 0.908\n",
      "train loss 0.143 valid loss 0.279 accuracy 0.908\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, 10, lr=0.005, wd=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.142 valid loss 0.279 accuracy 0.908\n",
      "train loss 0.142 valid loss 0.278 accuracy 0.908\n",
      "train loss 0.142 valid loss 0.278 accuracy 0.908\n",
      "train loss 0.142 valid loss 0.278 accuracy 0.908\n",
      "train loss 0.142 valid loss 0.278 accuracy 0.908\n",
      "train loss 0.142 valid loss 0.278 accuracy 0.908\n",
      "train loss 0.142 valid loss 0.278 accuracy 0.908\n",
      "train loss 0.142 valid loss 0.278 accuracy 0.908\n",
      "train loss 0.142 valid loss 0.277 accuracy 0.908\n",
      "train loss 0.142 valid loss 0.277 accuracy 0.908\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, 10, lr=0.005, wd=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.141 valid loss 0.277 accuracy 0.908\n",
      "train loss 0.141 valid loss 0.277 accuracy 0.908\n",
      "train loss 0.141 valid loss 0.277 accuracy 0.908\n",
      "train loss 0.141 valid loss 0.277 accuracy 0.908\n",
      "train loss 0.141 valid loss 0.277 accuracy 0.908\n",
      "train loss 0.141 valid loss 0.277 accuracy 0.909\n",
      "train loss 0.141 valid loss 0.277 accuracy 0.909\n",
      "train loss 0.141 valid loss 0.277 accuracy 0.909\n",
      "train loss 0.141 valid loss 0.276 accuracy 0.909\n",
      "train loss 0.141 valid loss 0.276 accuracy 0.909\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, 10, lr=0.005, wd=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.141 valid loss 0.276 accuracy 0.909\n",
      "train loss 0.141 valid loss 0.276 accuracy 0.909\n",
      "train loss 0.141 valid loss 0.276 accuracy 0.909\n",
      "train loss 0.141 valid loss 0.276 accuracy 0.909\n",
      "train loss 0.141 valid loss 0.276 accuracy 0.909\n",
      "train loss 0.140 valid loss 0.276 accuracy 0.909\n",
      "train loss 0.140 valid loss 0.276 accuracy 0.909\n",
      "train loss 0.140 valid loss 0.276 accuracy 0.909\n",
      "train loss 0.140 valid loss 0.276 accuracy 0.909\n",
      "train loss 0.140 valid loss 0.276 accuracy 0.909\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, 10, lr=0.005, wd=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.140 valid loss 0.276 accuracy 0.909\n",
      "train loss 0.140 valid loss 0.276 accuracy 0.909\n",
      "train loss 0.140 valid loss 0.275 accuracy 0.909\n",
      "train loss 0.140 valid loss 0.275 accuracy 0.909\n",
      "train loss 0.140 valid loss 0.275 accuracy 0.909\n",
      "train loss 0.140 valid loss 0.275 accuracy 0.909\n",
      "train loss 0.140 valid loss 0.275 accuracy 0.909\n",
      "train loss 0.140 valid loss 0.275 accuracy 0.909\n",
      "train loss 0.140 valid loss 0.275 accuracy 0.909\n",
      "train loss 0.140 valid loss 0.275 accuracy 0.909\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, 10, lr=0.005, wd=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.169 valid loss 0.299 accuracy 0.901\n",
      "train loss 0.168 valid loss 0.298 accuracy 0.901\n",
      "train loss 0.168 valid loss 0.298 accuracy 0.901\n",
      "train loss 0.167 valid loss 0.297 accuracy 0.901\n",
      "train loss 0.167 valid loss 0.297 accuracy 0.902\n",
      "train loss 0.166 valid loss 0.297 accuracy 0.902\n",
      "train loss 0.165 valid loss 0.296 accuracy 0.902\n",
      "train loss 0.165 valid loss 0.296 accuracy 0.902\n",
      "train loss 0.164 valid loss 0.295 accuracy 0.902\n",
      "train loss 0.164 valid loss 0.295 accuracy 0.902\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, 10, lr=0.005, wd=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.163 valid loss 0.295 accuracy 0.903\n",
      "train loss 0.163 valid loss 0.294 accuracy 0.903\n",
      "train loss 0.162 valid loss 0.294 accuracy 0.903\n",
      "train loss 0.162 valid loss 0.294 accuracy 0.903\n",
      "train loss 0.161 valid loss 0.293 accuracy 0.903\n",
      "train loss 0.161 valid loss 0.293 accuracy 0.903\n",
      "train loss 0.160 valid loss 0.292 accuracy 0.903\n",
      "train loss 0.160 valid loss 0.292 accuracy 0.903\n",
      "train loss 0.159 valid loss 0.292 accuracy 0.904\n",
      "train loss 0.159 valid loss 0.291 accuracy 0.904\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, 10, lr=0.005, wd=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.695 valid loss 0.684 accuracy 0.607\n",
      "train loss 0.605 valid loss 0.413 accuracy 0.850\n",
      "train loss 0.293 valid loss 0.442 accuracy 0.847\n",
      "train loss 0.210 valid loss 0.478 accuracy 0.845\n",
      "train loss 0.131 valid loss 0.496 accuracy 0.845\n",
      "train loss 0.072 valid loss 0.514 accuracy 0.849\n",
      "train loss 0.037 valid loss 0.544 accuracy 0.851\n",
      "train loss 0.019 valid loss 0.586 accuracy 0.851\n",
      "train loss 0.010 valid loss 0.639 accuracy 0.852\n",
      "train loss 0.005 valid loss 0.696 accuracy 0.852\n"
     ]
    }
   ],
   "source": [
    "# ============================ #\n",
    "# HYPER PARAMETER EXPERIMENTATION\n",
    "# ============================ #\n",
    "model2 = MF_bias(num_users, num_items, emb_size=200)\n",
    "train_epocs(model2, epochs=10, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.695 valid loss 0.681 accuracy 0.500\n",
      "train loss 0.671 valid loss 0.665 accuracy 0.591\n",
      "train loss 0.646 valid loss 0.643 accuracy 0.843\n",
      "train loss 0.617 valid loss 0.614 accuracy 0.857\n",
      "train loss 0.582 valid loss 0.579 accuracy 0.857\n",
      "train loss 0.541 valid loss 0.541 accuracy 0.856\n",
      "train loss 0.498 valid loss 0.501 accuracy 0.857\n",
      "train loss 0.454 valid loss 0.462 accuracy 0.858\n",
      "train loss 0.411 valid loss 0.429 accuracy 0.858\n",
      "train loss 0.374 valid loss 0.402 accuracy 0.858\n"
     ]
    }
   ],
   "source": [
    "model3 = MF_bias(num_users, num_items, emb_size=200)\n",
    "train_epocs(model3, epochs=10, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.342 valid loss 0.390 accuracy 0.858\n",
      "train loss 0.327 valid loss 0.380 accuracy 0.858\n",
      "train loss 0.313 valid loss 0.372 accuracy 0.858\n",
      "train loss 0.300 valid loss 0.366 accuracy 0.858\n",
      "train loss 0.289 valid loss 0.360 accuracy 0.857\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model3, epochs=5, lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.279 valid loss 0.356 accuracy 0.857\n",
      "train loss 0.269 valid loss 0.353 accuracy 0.857\n",
      "train loss 0.259 valid loss 0.351 accuracy 0.857\n",
      "train loss 0.250 valid loss 0.349 accuracy 0.857\n",
      "train loss 0.242 valid loss 0.348 accuracy 0.857\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model3, epochs=5, lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.234 valid loss 0.347 accuracy 0.858\n",
      "train loss 0.225 valid loss 0.347 accuracy 0.858\n",
      "train loss 0.217 valid loss 0.346 accuracy 0.858\n",
      "train loss 0.209 valid loss 0.346 accuracy 0.858\n",
      "train loss 0.202 valid loss 0.347 accuracy 0.858\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model3, epochs=5, lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.705 valid loss 0.671 accuracy 0.500\n",
      "train loss 0.654 valid loss 0.641 accuracy 0.503\n",
      "train loss 0.610 valid loss 0.603 accuracy 0.855\n",
      "train loss 0.561 valid loss 0.556 accuracy 0.859\n",
      "train loss 0.505 valid loss 0.504 accuracy 0.857\n",
      "train loss 0.446 valid loss 0.455 accuracy 0.857\n",
      "train loss 0.391 valid loss 0.415 accuracy 0.858\n",
      "train loss 0.343 valid loss 0.387 accuracy 0.858\n",
      "train loss 0.306 valid loss 0.372 accuracy 0.858\n",
      "train loss 0.279 valid loss 0.368 accuracy 0.858\n",
      "train loss 0.258 valid loss 0.370 accuracy 0.858\n",
      "train loss 0.241 valid loss 0.374 accuracy 0.858\n",
      "train loss 0.226 valid loss 0.380 accuracy 0.858\n",
      "train loss 0.210 valid loss 0.385 accuracy 0.858\n",
      "train loss 0.194 valid loss 0.390 accuracy 0.858\n"
     ]
    }
   ],
   "source": [
    "model4 = MF_bias(num_users, num_items, emb_size=500)\n",
    "train_epocs(model4, epochs=15, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained!\n"
     ]
    }
   ],
   "source": [
    "#train on full data\n",
    "model = MF_bias(num_users+1, num_items+1, emb_size=25)\n",
    "train_epocs_full(model, 80, 0.1, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained!\n"
     ]
    }
   ],
   "source": [
    "train_epocs_full(model, 260, lr=0.005, wd=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = training_df['user_id'].max()\n",
    "num_items = training_df['item_id'].unique().max()\n",
    "model = MF_bias(num_users+1, num_items+1, emb_size=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained!\n"
     ]
    }
   ],
   "source": [
    "train_epocs_full(model, 100, 0.1, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained!\n"
     ]
    }
   ],
   "source": [
    "train_epocs_full(model, 150, 0.005, 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 3 Experimentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF_features_bias(nn.Module):\n",
    "    def __init__(self, num_users, num_items, num_context, num_item_features, emb_size=100, feat_emb_size=50, seed=None):\n",
    "        super(MF_features_bias, self).__init__()\n",
    "        if seed is None:\n",
    "            torch.manual_seed(np.random.randint(1,100000))\n",
    "        else:\n",
    "            torch.manual_seed(seed)\n",
    "        self.user_emb = nn.Embedding(num_users, emb_size)\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_size)\n",
    "        self.item_bias = nn.Embedding(num_items, 1)\n",
    "        self.context_emb = nn.Embedding(num_context, feat_emb_size)\n",
    "        self.context_bias = nn.Embedding(num_context, 1)\n",
    "        self.item_feature_emb = nn.Embedding(num_item_features, feat_emb_size)\n",
    "        self.item_feature_bias = nn.Embedding(num_item_features, 1)\n",
    "        \n",
    "        self.user_emb.weight.data.uniform_(0,0.05)\n",
    "        self.item_emb.weight.data.uniform_(0,0.05)\n",
    "        self.context_emb.weight.data.uniform_(0,0.05)\n",
    "        self.item_feature_emb.weight.data.uniform_(0,0.05)\n",
    "        self.user_bias.weight.data.uniform_(-0.01,0.01)\n",
    "        self.item_bias.weight.data.uniform_(-0.01,0.01)\n",
    "        self.context_bias.weight.data.uniform_(-0.01,0.01)\n",
    "        self.item_feature_bias.weight.data.uniform_(-0.01,0.01)\n",
    "\n",
    "    def forward(self, u, v, c, i):\n",
    "        U = self.user_emb(u)\n",
    "        V = self.item_emb(v)\n",
    "        C = self.context_emb(c)\n",
    "        I = self.item_feature_emb(i)\n",
    "        b_u = self.user_bias(u).squeeze()\n",
    "        b_v = self.item_bias(v).squeeze()\n",
    "        b_c = self.user_bias(u).squeeze()\n",
    "        b_i = self.item_bias(v).squeeze()\n",
    "        return ((U*V).sum(1) + ((C*I).sum(1)) + b_u + b_v + b_c + b_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epocs(model, epochs=10, lr=0.01, wd=0.0):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        users = torch.LongTensor(train_df.user_id.values)  \n",
    "        items = torch.LongTensor(train_df.item_id.values) \n",
    "        context_features = torch.LongTensor(train_df.context_feature_id.values)  \n",
    "        item_features = torch.LongTensor(train_df.item_feature_id.values) \n",
    "        ratings = torch.FloatTensor(train_df.rating.values)  \n",
    "    \n",
    "        y_hat = model(users, items, context_features, item_features)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, ratings)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        testloss, acc = valid_loss(model)\n",
    "        print(\"train loss %.3f valid loss %.3f accuracy %.3f\" % (loss.item(), testloss, acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_loss(model):\n",
    "    model.eval()\n",
    "    users = torch.LongTensor(valid_df.user_id.values)  \n",
    "    items = torch.LongTensor(valid_df.item_id.values)\n",
    "    context_features = torch.LongTensor(valid_df.context_feature_id.values)  \n",
    "    item_features = torch.LongTensor(valid_df.item_feature_id.values) \n",
    "    ratings = torch.FloatTensor(valid_df.rating.values)  \n",
    "    y_hat = model(users, items, context_features, item_features)\n",
    "    loss = F.binary_cross_entropy_with_logits(y_hat, ratings)\n",
    "    preds = (torch.sigmoid(y_hat) > 0.5).float()\n",
    "    return loss.item(), balanced_accuracy_score(ratings, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>context_feature_id</th>\n",
       "      <th>item_feature_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63411</td>\n",
       "      <td>22331</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95998</td>\n",
       "      <td>22331</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118432</td>\n",
       "      <td>22331</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>127335</td>\n",
       "      <td>22331</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>14064</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194074</th>\n",
       "      <td>189501</td>\n",
       "      <td>38356</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194075</th>\n",
       "      <td>194022</td>\n",
       "      <td>38179</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194076</th>\n",
       "      <td>194817</td>\n",
       "      <td>38088</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194077</th>\n",
       "      <td>197009</td>\n",
       "      <td>39877</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194078</th>\n",
       "      <td>199208</td>\n",
       "      <td>38350</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194079 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id  item_id  rating  context_feature_id  item_feature_id\n",
       "0         63411    22331       0                   2              124\n",
       "1         95998    22331       0                   0              124\n",
       "2        118432    22331       0                   2              124\n",
       "3        127335    22331       1                   1              124\n",
       "4             0    14064       0                   3               75\n",
       "...         ...      ...     ...                 ...              ...\n",
       "194074   189501    38356       1                   2               63\n",
       "194075   194022    38179       1                   2               33\n",
       "194076   194817    38088       1                   0              148\n",
       "194077   197009    39877       1                   3              173\n",
       "194078   199208    38350       1                   2              148\n",
       "\n",
       "[194079 rows x 5 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = training_df['user_id'].max()\n",
    "num_items = training_df['item_id'].unique().max()\n",
    "num_context = training_df['context_feature_id'].max()\n",
    "num_item_features = training_df['item_feature_id'].max()\n",
    "model = MF_features_bias(num_users+1, num_items+1, num_context+1, num_item_features+1, emb_size=25, feat_emb_size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.677 valid loss 0.552 accuracy 0.783\n",
      "train loss 0.545 valid loss 0.507 accuracy 0.810\n",
      "train loss 0.505 valid loss 0.470 accuracy 0.847\n",
      "train loss 0.456 valid loss 0.442 accuracy 0.851\n",
      "train loss 0.415 valid loss 0.425 accuracy 0.853\n",
      "train loss 0.390 valid loss 0.413 accuracy 0.856\n",
      "train loss 0.376 valid loss 0.397 accuracy 0.861\n",
      "train loss 0.361 valid loss 0.380 accuracy 0.866\n",
      "train loss 0.348 valid loss 0.367 accuracy 0.868\n",
      "train loss 0.339 valid loss 0.357 accuracy 0.869\n",
      "train loss 0.332 valid loss 0.352 accuracy 0.869\n",
      "train loss 0.329 valid loss 0.349 accuracy 0.870\n",
      "train loss 0.327 valid loss 0.347 accuracy 0.870\n",
      "train loss 0.325 valid loss 0.346 accuracy 0.869\n",
      "train loss 0.323 valid loss 0.347 accuracy 0.868\n",
      "train loss 0.322 valid loss 0.348 accuracy 0.867\n",
      "train loss 0.321 valid loss 0.349 accuracy 0.867\n",
      "train loss 0.321 valid loss 0.350 accuracy 0.866\n",
      "train loss 0.321 valid loss 0.350 accuracy 0.865\n",
      "train loss 0.321 valid loss 0.349 accuracy 0.866\n",
      "train loss 0.321 valid loss 0.348 accuracy 0.866\n",
      "train loss 0.320 valid loss 0.347 accuracy 0.867\n",
      "train loss 0.320 valid loss 0.346 accuracy 0.868\n",
      "train loss 0.320 valid loss 0.345 accuracy 0.868\n",
      "train loss 0.320 valid loss 0.344 accuracy 0.868\n",
      "train loss 0.320 valid loss 0.343 accuracy 0.867\n",
      "train loss 0.319 valid loss 0.342 accuracy 0.868\n",
      "train loss 0.319 valid loss 0.341 accuracy 0.868\n",
      "train loss 0.318 valid loss 0.340 accuracy 0.868\n",
      "train loss 0.317 valid loss 0.339 accuracy 0.868\n",
      "train loss 0.316 valid loss 0.338 accuracy 0.868\n",
      "train loss 0.315 valid loss 0.337 accuracy 0.868\n",
      "train loss 0.314 valid loss 0.336 accuracy 0.868\n",
      "train loss 0.313 valid loss 0.335 accuracy 0.868\n",
      "train loss 0.312 valid loss 0.334 accuracy 0.867\n",
      "train loss 0.311 valid loss 0.334 accuracy 0.867\n",
      "train loss 0.310 valid loss 0.333 accuracy 0.867\n",
      "train loss 0.309 valid loss 0.333 accuracy 0.868\n",
      "train loss 0.309 valid loss 0.332 accuracy 0.868\n",
      "train loss 0.308 valid loss 0.332 accuracy 0.867\n",
      "train loss 0.308 valid loss 0.332 accuracy 0.867\n",
      "train loss 0.307 valid loss 0.332 accuracy 0.867\n",
      "train loss 0.307 valid loss 0.331 accuracy 0.868\n",
      "train loss 0.307 valid loss 0.331 accuracy 0.868\n",
      "train loss 0.307 valid loss 0.331 accuracy 0.867\n",
      "train loss 0.307 valid loss 0.331 accuracy 0.868\n",
      "train loss 0.307 valid loss 0.331 accuracy 0.868\n",
      "train loss 0.307 valid loss 0.331 accuracy 0.868\n",
      "train loss 0.307 valid loss 0.331 accuracy 0.868\n",
      "train loss 0.307 valid loss 0.331 accuracy 0.868\n",
      "train loss 0.307 valid loss 0.332 accuracy 0.868\n",
      "train loss 0.307 valid loss 0.332 accuracy 0.868\n",
      "train loss 0.307 valid loss 0.332 accuracy 0.868\n",
      "train loss 0.307 valid loss 0.332 accuracy 0.868\n",
      "train loss 0.307 valid loss 0.332 accuracy 0.868\n",
      "train loss 0.307 valid loss 0.332 accuracy 0.868\n",
      "train loss 0.308 valid loss 0.332 accuracy 0.868\n",
      "train loss 0.308 valid loss 0.332 accuracy 0.868\n",
      "train loss 0.308 valid loss 0.332 accuracy 0.868\n",
      "train loss 0.308 valid loss 0.332 accuracy 0.868\n",
      "train loss 0.308 valid loss 0.332 accuracy 0.868\n",
      "train loss 0.308 valid loss 0.332 accuracy 0.868\n",
      "train loss 0.308 valid loss 0.332 accuracy 0.868\n",
      "train loss 0.308 valid loss 0.332 accuracy 0.868\n",
      "train loss 0.308 valid loss 0.332 accuracy 0.868\n",
      "train loss 0.308 valid loss 0.332 accuracy 0.868\n",
      "train loss 0.308 valid loss 0.332 accuracy 0.868\n",
      "train loss 0.307 valid loss 0.332 accuracy 0.868\n",
      "train loss 0.308 valid loss 0.332 accuracy 0.868\n",
      "train loss 0.308 valid loss 0.332 accuracy 0.867\n",
      "train loss 0.308 valid loss 0.332 accuracy 0.868\n",
      "train loss 0.308 valid loss 0.332 accuracy 0.867\n",
      "train loss 0.308 valid loss 0.332 accuracy 0.868\n",
      "train loss 0.307 valid loss 0.332 accuracy 0.868\n",
      "train loss 0.308 valid loss 0.332 accuracy 0.867\n",
      "train loss 0.308 valid loss 0.332 accuracy 0.868\n",
      "train loss 0.307 valid loss 0.332 accuracy 0.868\n",
      "train loss 0.307 valid loss 0.332 accuracy 0.867\n",
      "train loss 0.308 valid loss 0.332 accuracy 0.868\n",
      "train loss 0.307 valid loss 0.332 accuracy 0.868\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, 80, 0.1, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.307 valid loss 0.331 accuracy 0.868\n",
      "train loss 0.306 valid loss 0.330 accuracy 0.868\n",
      "train loss 0.305 valid loss 0.330 accuracy 0.868\n",
      "train loss 0.304 valid loss 0.329 accuracy 0.868\n",
      "train loss 0.303 valid loss 0.329 accuracy 0.868\n",
      "train loss 0.302 valid loss 0.329 accuracy 0.868\n",
      "train loss 0.301 valid loss 0.328 accuracy 0.869\n",
      "train loss 0.300 valid loss 0.328 accuracy 0.869\n",
      "train loss 0.299 valid loss 0.327 accuracy 0.869\n",
      "train loss 0.298 valid loss 0.327 accuracy 0.869\n",
      "train loss 0.297 valid loss 0.327 accuracy 0.869\n",
      "train loss 0.296 valid loss 0.326 accuracy 0.869\n",
      "train loss 0.295 valid loss 0.326 accuracy 0.869\n",
      "train loss 0.294 valid loss 0.326 accuracy 0.869\n",
      "train loss 0.293 valid loss 0.325 accuracy 0.869\n",
      "train loss 0.292 valid loss 0.325 accuracy 0.869\n",
      "train loss 0.291 valid loss 0.325 accuracy 0.869\n",
      "train loss 0.290 valid loss 0.324 accuracy 0.870\n",
      "train loss 0.289 valid loss 0.324 accuracy 0.870\n",
      "train loss 0.288 valid loss 0.324 accuracy 0.870\n",
      "train loss 0.287 valid loss 0.324 accuracy 0.870\n",
      "train loss 0.286 valid loss 0.323 accuracy 0.870\n",
      "train loss 0.285 valid loss 0.323 accuracy 0.870\n",
      "train loss 0.284 valid loss 0.323 accuracy 0.870\n",
      "train loss 0.282 valid loss 0.322 accuracy 0.870\n",
      "train loss 0.281 valid loss 0.322 accuracy 0.870\n",
      "train loss 0.280 valid loss 0.322 accuracy 0.870\n",
      "train loss 0.279 valid loss 0.322 accuracy 0.871\n",
      "train loss 0.277 valid loss 0.321 accuracy 0.871\n",
      "train loss 0.276 valid loss 0.321 accuracy 0.871\n",
      "train loss 0.275 valid loss 0.321 accuracy 0.871\n",
      "train loss 0.273 valid loss 0.320 accuracy 0.871\n",
      "train loss 0.272 valid loss 0.320 accuracy 0.871\n",
      "train loss 0.270 valid loss 0.320 accuracy 0.872\n",
      "train loss 0.268 valid loss 0.319 accuracy 0.872\n",
      "train loss 0.266 valid loss 0.319 accuracy 0.872\n",
      "train loss 0.265 valid loss 0.318 accuracy 0.872\n",
      "train loss 0.263 valid loss 0.318 accuracy 0.872\n",
      "train loss 0.261 valid loss 0.318 accuracy 0.872\n",
      "train loss 0.259 valid loss 0.317 accuracy 0.873\n",
      "train loss 0.257 valid loss 0.317 accuracy 0.873\n",
      "train loss 0.255 valid loss 0.316 accuracy 0.873\n",
      "train loss 0.252 valid loss 0.316 accuracy 0.873\n",
      "train loss 0.250 valid loss 0.316 accuracy 0.874\n",
      "train loss 0.248 valid loss 0.315 accuracy 0.874\n",
      "train loss 0.246 valid loss 0.315 accuracy 0.874\n",
      "train loss 0.243 valid loss 0.314 accuracy 0.874\n",
      "train loss 0.241 valid loss 0.314 accuracy 0.874\n",
      "train loss 0.238 valid loss 0.313 accuracy 0.875\n",
      "train loss 0.236 valid loss 0.313 accuracy 0.875\n",
      "train loss 0.233 valid loss 0.313 accuracy 0.875\n",
      "train loss 0.231 valid loss 0.312 accuracy 0.875\n",
      "train loss 0.228 valid loss 0.312 accuracy 0.875\n",
      "train loss 0.225 valid loss 0.311 accuracy 0.876\n",
      "train loss 0.223 valid loss 0.311 accuracy 0.876\n",
      "train loss 0.220 valid loss 0.310 accuracy 0.876\n",
      "train loss 0.217 valid loss 0.310 accuracy 0.876\n",
      "train loss 0.215 valid loss 0.310 accuracy 0.877\n",
      "train loss 0.212 valid loss 0.309 accuracy 0.877\n",
      "train loss 0.210 valid loss 0.309 accuracy 0.877\n",
      "train loss 0.207 valid loss 0.308 accuracy 0.877\n",
      "train loss 0.204 valid loss 0.308 accuracy 0.878\n",
      "train loss 0.202 valid loss 0.308 accuracy 0.878\n",
      "train loss 0.199 valid loss 0.307 accuracy 0.878\n",
      "train loss 0.197 valid loss 0.307 accuracy 0.878\n",
      "train loss 0.194 valid loss 0.306 accuracy 0.879\n",
      "train loss 0.192 valid loss 0.306 accuracy 0.879\n",
      "train loss 0.190 valid loss 0.306 accuracy 0.879\n",
      "train loss 0.187 valid loss 0.305 accuracy 0.879\n",
      "train loss 0.185 valid loss 0.305 accuracy 0.880\n",
      "train loss 0.183 valid loss 0.304 accuracy 0.880\n",
      "train loss 0.181 valid loss 0.304 accuracy 0.880\n",
      "train loss 0.178 valid loss 0.304 accuracy 0.880\n",
      "train loss 0.176 valid loss 0.303 accuracy 0.881\n",
      "train loss 0.174 valid loss 0.303 accuracy 0.881\n",
      "train loss 0.172 valid loss 0.303 accuracy 0.881\n",
      "train loss 0.170 valid loss 0.302 accuracy 0.881\n",
      "train loss 0.169 valid loss 0.302 accuracy 0.881\n",
      "train loss 0.167 valid loss 0.301 accuracy 0.882\n",
      "train loss 0.165 valid loss 0.301 accuracy 0.882\n",
      "train loss 0.163 valid loss 0.301 accuracy 0.882\n",
      "train loss 0.162 valid loss 0.300 accuracy 0.882\n",
      "train loss 0.160 valid loss 0.300 accuracy 0.882\n",
      "train loss 0.159 valid loss 0.300 accuracy 0.882\n",
      "train loss 0.157 valid loss 0.300 accuracy 0.883\n",
      "train loss 0.156 valid loss 0.299 accuracy 0.883\n",
      "train loss 0.154 valid loss 0.299 accuracy 0.883\n",
      "train loss 0.153 valid loss 0.299 accuracy 0.883\n",
      "train loss 0.152 valid loss 0.298 accuracy 0.883\n",
      "train loss 0.151 valid loss 0.298 accuracy 0.883\n",
      "train loss 0.149 valid loss 0.298 accuracy 0.883\n",
      "train loss 0.148 valid loss 0.297 accuracy 0.884\n",
      "train loss 0.147 valid loss 0.297 accuracy 0.884\n",
      "train loss 0.146 valid loss 0.297 accuracy 0.884\n",
      "train loss 0.145 valid loss 0.297 accuracy 0.884\n",
      "train loss 0.144 valid loss 0.296 accuracy 0.884\n",
      "train loss 0.144 valid loss 0.296 accuracy 0.884\n",
      "train loss 0.143 valid loss 0.296 accuracy 0.885\n",
      "train loss 0.142 valid loss 0.296 accuracy 0.885\n",
      "train loss 0.141 valid loss 0.295 accuracy 0.885\n",
      "train loss 0.140 valid loss 0.295 accuracy 0.885\n",
      "train loss 0.140 valid loss 0.295 accuracy 0.885\n",
      "train loss 0.139 valid loss 0.295 accuracy 0.885\n",
      "train loss 0.138 valid loss 0.295 accuracy 0.885\n",
      "train loss 0.138 valid loss 0.294 accuracy 0.885\n",
      "train loss 0.137 valid loss 0.294 accuracy 0.886\n",
      "train loss 0.137 valid loss 0.294 accuracy 0.886\n",
      "train loss 0.136 valid loss 0.294 accuracy 0.886\n",
      "train loss 0.135 valid loss 0.293 accuracy 0.886\n",
      "train loss 0.135 valid loss 0.293 accuracy 0.886\n",
      "train loss 0.134 valid loss 0.293 accuracy 0.886\n",
      "train loss 0.134 valid loss 0.293 accuracy 0.886\n",
      "train loss 0.133 valid loss 0.293 accuracy 0.886\n",
      "train loss 0.133 valid loss 0.292 accuracy 0.886\n",
      "train loss 0.133 valid loss 0.292 accuracy 0.886\n",
      "train loss 0.132 valid loss 0.292 accuracy 0.887\n",
      "train loss 0.132 valid loss 0.292 accuracy 0.887\n",
      "train loss 0.131 valid loss 0.292 accuracy 0.887\n",
      "train loss 0.131 valid loss 0.291 accuracy 0.887\n",
      "train loss 0.131 valid loss 0.291 accuracy 0.887\n",
      "train loss 0.130 valid loss 0.291 accuracy 0.887\n",
      "train loss 0.130 valid loss 0.291 accuracy 0.887\n",
      "train loss 0.129 valid loss 0.291 accuracy 0.887\n",
      "train loss 0.129 valid loss 0.290 accuracy 0.887\n",
      "train loss 0.129 valid loss 0.290 accuracy 0.887\n",
      "train loss 0.128 valid loss 0.290 accuracy 0.888\n",
      "train loss 0.128 valid loss 0.290 accuracy 0.888\n",
      "train loss 0.128 valid loss 0.290 accuracy 0.888\n",
      "train loss 0.127 valid loss 0.290 accuracy 0.888\n",
      "train loss 0.127 valid loss 0.289 accuracy 0.888\n",
      "train loss 0.127 valid loss 0.289 accuracy 0.888\n",
      "train loss 0.127 valid loss 0.289 accuracy 0.888\n",
      "train loss 0.126 valid loss 0.289 accuracy 0.888\n",
      "train loss 0.126 valid loss 0.289 accuracy 0.888\n",
      "train loss 0.126 valid loss 0.289 accuracy 0.888\n",
      "train loss 0.125 valid loss 0.288 accuracy 0.888\n",
      "train loss 0.125 valid loss 0.288 accuracy 0.888\n",
      "train loss 0.125 valid loss 0.288 accuracy 0.888\n",
      "train loss 0.125 valid loss 0.288 accuracy 0.888\n",
      "train loss 0.124 valid loss 0.288 accuracy 0.889\n",
      "train loss 0.124 valid loss 0.288 accuracy 0.889\n",
      "train loss 0.124 valid loss 0.287 accuracy 0.889\n",
      "train loss 0.124 valid loss 0.287 accuracy 0.889\n",
      "train loss 0.123 valid loss 0.287 accuracy 0.889\n",
      "train loss 0.123 valid loss 0.287 accuracy 0.889\n",
      "train loss 0.123 valid loss 0.287 accuracy 0.889\n",
      "train loss 0.123 valid loss 0.287 accuracy 0.889\n",
      "train loss 0.123 valid loss 0.287 accuracy 0.889\n",
      "train loss 0.122 valid loss 0.286 accuracy 0.889\n",
      "train loss 0.122 valid loss 0.286 accuracy 0.889\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, 150, lr=0.005, wd=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Averaging Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6427675692269349"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_1 = pd.read_csv('submission.csv')\n",
    "pred_2 = pd.read_csv('submission_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting for Observed Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "test = pd.read_csv('test_kaggle.csv')\n",
    "test_ids = test['id']\n",
    "items_id_df = pd.read_csv('item_feature.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.merge(items_id_df, left_on='item_id', right_on='item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = torch.LongTensor(test.user_id.values)  \n",
    "items = torch.LongTensor(test.item_id.values)\n",
    "context_features = torch.LongTensor(test.context_feature_id.values)  \n",
    "item_features = torch.LongTensor(test.item_feature_id.values) \n",
    "\n",
    "y_hat_final = torch.sigmoid(model(users, items, context_features, item_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = torch.LongTensor(test.user_id.values)  \n",
    "items = torch.LongTensor(test.item_id.values)\n",
    "\n",
    "y_hat_final = torch.sigmoid(model(users, items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.604041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.414341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.847360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.275295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.230527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381380</th>\n",
       "      <td>0.781686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381381</th>\n",
       "      <td>0.932825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381382</th>\n",
       "      <td>0.892413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381383</th>\n",
       "      <td>0.892413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381384</th>\n",
       "      <td>0.269228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>381385 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          rating\n",
       "id              \n",
       "0       0.604041\n",
       "1       0.414341\n",
       "2       0.847360\n",
       "3       0.275295\n",
       "4       0.230527\n",
       "...          ...\n",
       "381380  0.781686\n",
       "381381  0.932825\n",
       "381382  0.892413\n",
       "381383  0.892413\n",
       "381384  0.269228\n",
       "\n",
       "[381385 rows x 1 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_df = pd.DataFrame(list(y_hat_final.detach().numpy()), index=test_ids).rename({0: 'rating'}, axis='columns')\n",
    "kaggle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5824710726737976"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_df['rating'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_df.to_csv('submission_6.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Model 2\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('training.csv')\n",
    "train_df['rating'] = 1\n",
    "items_id_df = pd.read_csv('item_feature.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling = train_df.to_numpy()\n",
    "all_users = pd.unique(sampling[:,0])\n",
    "all_items = pd.unique(sampling[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_samples = add_samples(sampling, 1, all_users, all_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_feature_id_dict = {}\n",
    "for user in train_df['user_id'].unique():\n",
    "    context_feature_id_dict[user] = train_df[train_df['user_id'] == user]['context_feature_id'].sample(1).values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_samples['context_feature_id'] = negative_samples['user_id'].map(context_feature_id_dict)\n",
    "train_df_new = pd.concat([negative_samples, train_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>context_feature_id</th>\n",
       "      <th>item_feature_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>22331</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3323</td>\n",
       "      <td>22331</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6421</td>\n",
       "      <td>22331</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9189</td>\n",
       "      <td>22331</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19281</td>\n",
       "      <td>22331</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940485</th>\n",
       "      <td>198093</td>\n",
       "      <td>38949</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940486</th>\n",
       "      <td>198439</td>\n",
       "      <td>39733</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940487</th>\n",
       "      <td>198921</td>\n",
       "      <td>39831</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940488</th>\n",
       "      <td>198921</td>\n",
       "      <td>37989</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940489</th>\n",
       "      <td>199208</td>\n",
       "      <td>38350</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1940490 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  item_id  rating  context_feature_id  item_feature_id\n",
       "0              0    22331       0                   3              124\n",
       "1           3323    22331       0                   2              124\n",
       "2           6421    22331       0                   0              124\n",
       "3           9189    22331       0                   2              124\n",
       "4          19281    22331       0                   3              124\n",
       "...          ...      ...     ...                 ...              ...\n",
       "1940485   198093    38949       1                   1              142\n",
       "1940486   198439    39733       1                   2              138\n",
       "1940487   198921    39831       1                   2               94\n",
       "1940488   198921    37989       1                   2               94\n",
       "1940489   199208    38350       1                   2              148\n",
       "\n",
       "[1940490 rows x 5 columns]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df = train_df_new.merge(items_id_df, left_on='item_id', right_on='item_id')\n",
    "training_df = training_df.astype('int')\n",
    "training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.to_feather('training_sampled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epocs(model, epochs=10, lr=0.01, wd=0.0):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        users = torch.LongTensor(train_df_NN.user_id.values)  \n",
    "        items = torch.LongTensor(train_df_NN.item_id.values) \n",
    "        context_feature_id = torch.LongTensor(train_df_NN.context_feature_id.values)\n",
    "        item_feature_id = torch.LongTensor(train_df_NN.item_feature_id.values)\n",
    "        ratings = torch.FloatTensor(train_df_NN.rating.values)  \n",
    "    \n",
    "        y_hat = model(users, items, item_feature_id, context_feature_id)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, ratings)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        testloss, acc = valid_loss(model)\n",
    "        print(\"train loss %.3f valid loss %.3f accuracy %.3f\" % (loss.item(), testloss, acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_loss(model):\n",
    "    model.eval()\n",
    "    users = torch.LongTensor(test_df_NN.user_id.values)  \n",
    "    items = torch.LongTensor(test_df_NN.item_id.values) \n",
    "    context_feature_id = torch.LongTensor(test_df_NN.context_feature_id.values)\n",
    "    item_feature_id = torch.LongTensor(test_df_NN.item_feature_id.values)\n",
    "    ratings = torch.FloatTensor(test_df_NN.rating.values)\n",
    "\n",
    "    y_hat = model(users, items, item_feature_id, context_feature_id)\n",
    "    loss = F.binary_cross_entropy_with_logits(y_hat, ratings)\n",
    "    preds = (y_hat > 0.5).float()\n",
    "    return loss.item(), balanced_accuracy_score(ratings, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, num_users, num_items, num_item_features, num_context_features, emb_size=75, M=2):\n",
    "        super(NN, self).__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, emb_size)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_size)\n",
    "        self.item_feature_id = nn.Embedding(num_item_features, emb_size)\n",
    "        self.context_feature_id = nn.Embedding(num_context_features, emb_size)\n",
    "        self.hidden_layer = M\n",
    "        # initlializing weights\n",
    "        self.user_emb.weight.data.uniform_(0,0.05)\n",
    "        self.item_emb.weight.data.uniform_(0,0.05)\n",
    "        self.item_feature_id.weight.data.uniform_(0,0.05)\n",
    "        self.context_feature_id.weight.data.uniform_(0,0.05)\n",
    "        #layers\n",
    "        self.linear1 = nn.Linear(emb_size*4, M)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(M, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, u, v, i, c):\n",
    "        u = self.user_emb(u)\n",
    "        v = self.item_emb(v)\n",
    "        i = self.item_feature_id(i)\n",
    "        c = self.context_feature_id(c)\n",
    "        concat = torch.concat([u, v, i, c], axis=1)\n",
    "        pred = self.linear1(concat)\n",
    "        pred = self.relu(pred)\n",
    "        pred = self.linear2(pred)\n",
    "        return self.sig(pred.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_NN, test_df_NN = train_test_split(training_df)\n",
    "train_df_NN = train_df_NN.drop(columns=['index'])\n",
    "test_df_NN = test_df_NN.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df_NN.drop(columns='rating').values\n",
    "y_train = train_df_NN['rating']\n",
    "X_test = test_df_NN.drop(columns='rating').values\n",
    "y_test = test_df_NN['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = training_df['user_id'].max()\n",
    "num_items = training_df['item_id'].max()\n",
    "num_item_features = training_df['item_feature_id'].max()\n",
    "num_context_features = training_df['context_feature_id'].max()\n",
    "model = NN(num_users+1, num_items+1, num_item_features+1, num_context_features+1, emb_size=25, M=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.717 valid loss 0.712 accuracy 0.500\n",
      "train loss 0.713 valid loss 0.699 accuracy 0.500\n",
      "train loss 0.698 valid loss 0.681 accuracy 0.500\n",
      "train loss 0.679 valid loss 0.669 accuracy 0.500\n",
      "train loss 0.665 valid loss 0.659 accuracy 0.500\n",
      "train loss 0.653 valid loss 0.642 accuracy 0.500\n",
      "train loss 0.637 valid loss 0.636 accuracy 0.500\n",
      "train loss 0.634 valid loss 0.633 accuracy 0.500\n",
      "train loss 0.631 valid loss 0.631 accuracy 0.500\n",
      "train loss 0.629 valid loss 0.629 accuracy 0.500\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, lr=0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.571 valid loss 0.578 accuracy 0.859\n",
      "train loss 0.569 valid loss 0.576 accuracy 0.859\n",
      "train loss 0.568 valid loss 0.575 accuracy 0.859\n",
      "train loss 0.567 valid loss 0.574 accuracy 0.860\n",
      "train loss 0.566 valid loss 0.573 accuracy 0.860\n",
      "train loss 0.565 valid loss 0.572 accuracy 0.860\n",
      "train loss 0.565 valid loss 0.572 accuracy 0.860\n",
      "train loss 0.564 valid loss 0.571 accuracy 0.860\n",
      "train loss 0.564 valid loss 0.571 accuracy 0.860\n",
      "train loss 0.563 valid loss 0.570 accuracy 0.860\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(num_users+1, num_items+1, num_item_features+1, num_context_features+1, emb_size=25, M=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.719 valid loss 0.716 accuracy 0.500\n",
      "train loss 0.716 valid loss 0.711 accuracy 0.500\n",
      "train loss 0.711 valid loss 0.703 accuracy 0.500\n",
      "train loss 0.703 valid loss 0.688 accuracy 0.500\n",
      "train loss 0.688 valid loss 0.677 accuracy 0.500\n",
      "train loss 0.677 valid loss 0.674 accuracy 0.500\n",
      "train loss 0.674 valid loss 0.667 accuracy 0.500\n",
      "train loss 0.667 valid loss 0.657 accuracy 0.500\n",
      "train loss 0.657 valid loss 0.647 accuracy 0.542\n",
      "train loss 0.647 valid loss 0.632 accuracy 0.645\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, lr=0.1, wd=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.632 valid loss 0.615 accuracy 0.725\n",
      "train loss 0.615 valid loss 0.602 accuracy 0.749\n",
      "train loss 0.602 valid loss 0.592 accuracy 0.785\n",
      "train loss 0.592 valid loss 0.586 accuracy 0.801\n",
      "train loss 0.586 valid loss 0.581 accuracy 0.817\n",
      "train loss 0.580 valid loss 0.576 accuracy 0.829\n",
      "train loss 0.575 valid loss 0.572 accuracy 0.840\n",
      "train loss 0.571 valid loss 0.569 accuracy 0.845\n",
      "train loss 0.568 valid loss 0.568 accuracy 0.847\n",
      "train loss 0.567 valid loss 0.567 accuracy 0.851\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, lr=0.05, wd=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.565 valid loss 0.566 accuracy 0.853\n",
      "train loss 0.565 valid loss 0.567 accuracy 0.848\n",
      "train loss 0.564 valid loss 0.566 accuracy 0.851\n",
      "train loss 0.564 valid loss 0.566 accuracy 0.856\n",
      "train loss 0.563 valid loss 0.565 accuracy 0.859\n",
      "train loss 0.562 valid loss 0.565 accuracy 0.860\n",
      "train loss 0.561 valid loss 0.565 accuracy 0.860\n",
      "train loss 0.560 valid loss 0.564 accuracy 0.861\n",
      "train loss 0.560 valid loss 0.564 accuracy 0.863\n",
      "train loss 0.559 valid loss 0.564 accuracy 0.864\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, lr=0.01, wd=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.559 valid loss 0.566 accuracy 0.850\n",
      "train loss 0.561 valid loss 0.564 accuracy 0.863\n",
      "train loss 0.559 valid loss 0.564 accuracy 0.868\n",
      "train loss 0.559 valid loss 0.563 accuracy 0.868\n",
      "train loss 0.558 valid loss 0.563 accuracy 0.867\n",
      "train loss 0.557 valid loss 0.563 accuracy 0.865\n",
      "train loss 0.557 valid loss 0.563 accuracy 0.865\n",
      "train loss 0.557 valid loss 0.563 accuracy 0.866\n",
      "train loss 0.556 valid loss 0.563 accuracy 0.868\n",
      "train loss 0.556 valid loss 0.564 accuracy 0.869\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, lr=0.01, wd=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "test = pd.read_csv('test_kaggle.csv')\n",
    "test_ids = test['id']\n",
    "test = test.merge(items_id_df, left_on='item_id', right_on='item_id').drop(columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = torch.LongTensor(test.user_id.values)  \n",
    "items = torch.LongTensor(test.item_id.values) \n",
    "context_feature_id = torch.LongTensor(test.context_feature_id.values)\n",
    "item_feature_id = torch.LongTensor(test.item_feature_id.values)\n",
    "y_hat_final = model(users, items, item_feature_id, context_feature_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_df = pd.DataFrame(list(y_hat_final.detach().numpy()), index=test_ids).rename({0: 'rating'}, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_df.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb7718bc7a284688e4e0a6ba73d93065ef2522704feae8041a0a6436034cecc"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
